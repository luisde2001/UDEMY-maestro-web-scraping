{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bee98dc",
   "metadata": {},
   "source": [
    "# STACKOVERFLOW con Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8ed6e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "OBJETIVO: \n",
    "    - Extraer las preguntas de la pagina principal de Stackoverflow con Scrapy\n",
    "CREADO POR: LEONARDO KUFFO\n",
    "ULTIMA VEZ EDITADO: 09 ENERO 2023\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973a939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VER RECURSOS DE LA CLASE PARA INSTALAR SCRAPY\n",
    "from scrapy.item import Field\n",
    "from scrapy.item import Item\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.loader.processors import MapCompose\n",
    "from scrapy.loader import ItemLoader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d842549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scrapy.spiders import Spider\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00252270",
   "metadata": {},
   "source": [
    "Vamos a definir las clases que se necesitan en Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e0ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSTRACCION DE DATOS A EXTRAER - DETERMINA LOS DATOS QUE TENGO QUE LLENAR Y QUE ESTARAN EN EL ARCHIVO GENERADO\n",
    "class Pregunta(Item):\n",
    "    id = Field()\n",
    "    pregunta = Field()\n",
    "    #descripcion = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fe9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE CORE - SPIDER\n",
    "class StackOverflowSpider(Spider):\n",
    "    name = \"MiPrimerSpider\" # nombre, puede ser cualquiera \n",
    "    \n",
    "    # Forma de configurar el USER AGENT en scrapy\n",
    "    custom_settings = {\n",
    "        'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # URL SEMILLA\n",
    "    start_urls = ['https://stackoverflow.com/questions']\n",
    "    \n",
    "    # Funcion que se va a llamar cuando se haga el requerimiento a la URL semilla\n",
    "    def parse(self, response):\n",
    "        # Selectores: Clase de scrapy para extraer datos\n",
    "        sel = Selector(response) #Va a utilizar selectores de xpath\n",
    "        titulo_de_pagina = sel.xpath('//h1/text()').get()\n",
    "        print (titulo_de_pagina)\n",
    "        # Selector de varias preguntas\n",
    "        preguntas = sel.xpath('//div[@id=\"questions\"]//div[contains(@class,\"s-post-summary \")]') \n",
    "        i = 0\n",
    "        for pregunta in preguntas:\n",
    "            item = ItemLoader(Pregunta(), pregunta) # Instancio mi ITEM con el selector en donde estan los datos para llenarlo\n",
    "\n",
    "            # Lleno las propiedades de mi ITEM a traves de expresiones XPATH a buscar dentro del selector \"pregunta\"\n",
    "            item.add_xpath('pregunta', './/h3/a/text()') \n",
    "            # item.add_xpath('descripcion', './/div[@class=\"s-post-summary--content-excerpt\"]/text()')\n",
    "            item.add_value('id', i)\n",
    "            i += 1\n",
    "            yield item.load_item() # Hago Yield de la informacion para que se escriban los datos en el archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ae2c9",
   "metadata": {},
   "source": [
    "Ahora el problema es que no se puede correr como un notebook. Hay que correrlo por Terminal:  \n",
    "> scrapy runspider nombredelarchivo.py -o nombrenuevoarchivo.csv -t csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1949ad6",
   "metadata": {},
   "source": [
    "La forma de hacerlo es por medio de la carga de librer√≠a **from scrapy.crawler import CrawlerProcess**, y las siguientes instrucciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58f70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 19:40:19 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: scrapybot)\n",
      "2023-08-23 19:40:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.11.5, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:10:28) [Clang 15.0.7 ], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform macOS-12.6.8-x86_64-i386-64bit\n",
      "2023-08-23 19:40:19 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-08-23 19:40:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, '\n",
      "               'like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 '\n",
      "               'Safari/537.36'}\n",
      "2023-08-23 19:40:19 [py.warnings] WARNING: /opt/anaconda3/envs/webchanges/lib/python3.11/site-packages/scrapy/utils/request.py:248: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-08-23 19:40:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-08-23 19:40:20 [scrapy.extensions.telnet] INFO: Telnet Password: f922c508d654e923\n",
      "2023-08-23 19:40:20 [py.warnings] WARNING: /opt/anaconda3/envs/webchanges/lib/python3.11/site-packages/scrapy/extensions/feedexport.py:398: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2023-08-23 19:40:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-08-23 19:40:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-08-23 19:40:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-08-23 19:40:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-08-23 19:40:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-08-23 19:40:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-08-23 19:40:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-08-23 19:40:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/questions> (referer: None)\n",
      "2023-08-23 19:40:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [0], 'pregunta': ['What is an invalid LiveChatMessage text?']}\n",
      "2023-08-23 19:40:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [1],\n",
      " 'pregunta': ['how does backtracking for n times after the nth iteration in '\n",
      "              'bellman ford gurantees that a node of negative cycle found']}\n",
      "2023-08-23 19:40:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [2],\n",
      " 'pregunta': ['How to validate an array of data in Firebase Firestore?']}\n",
      "2023-08-23 19:40:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [3], 'pregunta': [\"Error: 'crypt' was not declared in this scope\"]}\n",
      "2023-08-23 19:40:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [4],\n",
      " 'pregunta': ['Properties are undefined when sending a POST request to an '\n",
      "              'express API']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [5], 'pregunta': ['How to init a final variable in Ballerina']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [6],\n",
      " 'pregunta': ['Creating dict groupby from Dataframe with node structure']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [7],\n",
      " 'pregunta': [\"Expo App is crashing in Apple's TestFlight version. It runs \"\n",
      "              'perfectly fine locally, even with npx expo start --no-dev '\n",
      "              '--minify']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [8],\n",
      " 'pregunta': ['C# .net validate if path in patch starts with slash / otherwise '\n",
      "              'throw error']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [9],\n",
      " 'pregunta': ['How to know if any blocking synchronous api requests are '\n",
      "              'getting called through DOM']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [10],\n",
      " 'pregunta': ['Conditional formatting/alert flag/icons in power apps tables']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [11], 'pregunta': ['Pyinstaller and SQLalchemy probelems']}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [12], 'pregunta': ['Keycloak client name issue']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [13],\n",
      " 'pregunta': ['Order of operations with expr1=expr2 where LHS is an '\n",
      "              'unordered_map value [duplicate]']}\n",
      "2023-08-23 19:40:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://stackoverflow.com/questions>\n",
      "{'id': [14],\n",
      " 'pregunta': ['windows update Updateorchestrator restart after installation of '\n",
      "              'updates Server 2016']}\n",
      "2023-08-23 19:40:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-08-23 19:40:21 [scrapy.extensions.feedexport] INFO: Stored csv feed (15 items) in: nombrenuevoarchivoipynb.csv\n",
      "2023-08-23 19:40:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 329,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 42352,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.95534,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 8, 23, 17, 40, 21, 188423),\n",
      " 'httpcompression/response_bytes': 189564,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 15,\n",
      " 'log_count/DEBUG': 17,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 2,\n",
      " 'memusage/max': 97312768,\n",
      " 'memusage/startup': 97312768,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 8, 23, 17, 40, 20, 233083)}\n",
      "2023-08-23 19:40:21 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'csv',\n",
    "    'FEED_URI':'nombrenuevoarchivoipynb.csv'\n",
    "})\n",
    "process.crawl(StackOverflowSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
